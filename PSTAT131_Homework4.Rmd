---
title: "PSTAT131 Homework 4"
author: "Shivani Kharva"
date: "2022-10-31"
output:
  html_document:
    toc: true
---

### Initial Setup
```{r, message = FALSE}
# Loading the data/ packages
titanic_data <- read.csv("data/titanic.csv")
library(tidymodels)
library(ISLR)
library(tidyverse)
library(discrim)
library(poissonreg)
tidymodels_prefer()
```

```{r}
# Changing `survived` and `pclass` to factors
survived_levels <- c("Yes", "No")

titanic_data$survived <- factor(titanic_data$survived, levels = survived_levels)
titanic_data$pclass <- as.factor(titanic_data$pclass)

# Setting the seed
set.seed(0124)
```

### Question 1  

```{r}
# Splitting the data and stratifying on the outcome, `survived`
titanic_split <- initial_split(titanic_data, prop = 0.70, strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```

```{r}
# Verifying that the training and testing data have the correct number of outcomes
nrow(titanic_train)/nrow(titanic_data)
nrow(titanic_test)/nrow(titanic_data)
```

The training data has \~70% of the observations of the original data set and the testing data has \~30% of the observations of the original data set.  

```{r}
# Creating recipe identical to Homework 3
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>% 
  # Using imputation to deal with missing age values
  step_impute_linear(age, impute_with = imp_vars(pclass, sex, sib_sp, parch, fare)) %>% 
  # Encoding categorical predictors
  step_dummy(all_nominal_predictors()) %>% 
  # Including interactions
  step_interact(terms = ~ starts_with("sex"):fare + age:fare)
```

### Question 2  
```{r}
# Creating the folds for using k-folds cross validation
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds
```

### Question 3  

In question 2, we are creating 10 folds to conduct k-fold (10-fold in our case) cross validation. This means that R is taking the training data and assigning each observation in the training data to 1 of 10 folds. For each fold, a testing set is created consisting of that fold and the remaining k-1 folds will be the training set for that fold. At the end, we end up with k total folds.  

K-fold cross validation is done by splitting the data into k folds as described above with each fold being a testing set with the other k-1 folds being the training set for that fold. Then, whichever model we are fitting is fit to each training set and tested on the corresponding testing set (each time, a different fold should be used as a validation set). Then, the average accuracy is taken from the testing set of each of the folds to measure performance (other metrics can be taken as well such as standard error).   

We use k-fold cross validation rather than simply fitting and testing models on the entire training set because cross validation provides a better estimate of the testing accuracy. It is better to take the mean accuracy from several samples instead of just one accuracy from one sample because, as n increases, we reduce variation.

According to the professor in lecture, the last question is supposed to ask which method creates a training set, one validation set, and a testing set. This would be the validation set approach.   

### Question 4  

```{r}
# Logistic Regression

# Specifying logistic regression model for classification using glm engine
log_reg <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")

# Creating workflow
log_workflow <- workflow() %>% 
  # Adding the model
  add_model(log_reg) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)
#-----------
# LDA

# Specifying linear discriminant analysis model using MASS engine
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# Creating workflow
lda_workflow <- workflow() %>% 
  # Adding the model
  add_model(lda_mod) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)
#-----------
# QDA

# Specifying quadratic discriminant analysis model using MASS engine
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# Creating workflow
qda_workflow <- workflow() %>% 
  # Adding the model
  add_model(qda_mod) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)
```

We will be fitting a total of 30 models (3 models * 10 folds) across all folds to the data.  

### Question 5  

```{r, eval=FALSE}
# Fitting the models to the folds
log_fit <- fit_resamples(log_workflow, resamples = titanic_folds)
lda_fit <- fit_resamples(lda_workflow, resamples = titanic_folds)
qda_fit <- fit_resamples(qda_workflow, resamples = titanic_folds)

# Saving the fitted models as rds files
write_rds(log_fit, file= "log_fit.rds")
write_rds(lda_fit, file= "lda_fit.rds")
write_rds(qda_fit, file = "qda_fit.rds")
```

### Question 6  

```{r}
# Loading in the saved fitted models
log_fit <- read_rds("log_fit.rds")
lda_fit <- read_rds("lda_fit.rds")
qda_fit <- read_rds("qda_fit.rds")

# Printing the mean and standards errors of accuracy
collect_metrics(log_fit)
collect_metrics(lda_fit)
collect_metrics(qda_fit)
```

The logistic regression model has performed the best among the 3 fitted models in terms of accuracy. QDA had both the lowest mean accuracy (~0.767) and highest standard error (~0.023) among the models so that is not the model we should choose. LDA had a lower mean accuracy (~0.787) than logistic regression (~0.792) by about 0.01, but also a lower standard error by about 0.004 (LDA: ~0.015 vs. logistic: ~0.019). In fact, LDA is within one standard error of the logistic regression mean accuracy. However, by rule of thumb (based on what Professor Coburn said in her office hours), if we can choose a simpler model, we should. So, LDA might work here as well since it is within one standard error of the logistic regression model and has a slightly lower standard error than logistic regression and a similar mean accuracy. But, it would be best to move forwards by fitting the logistic regression model to the training and test data because it is simpler and still a valid choice of model to use based on its mean accuracy and standard error in comparison to the other models.    

### Question 7  

```{r}
# Fitting logistic regression to the entire training set
log_training_fit <- fit(log_workflow, titanic_train)
```

### Question 8  
```{r}
# Generating predictions for LDA
log_tibble <- predict(log_training_fit, new_data = titanic_test %>% 
                        dplyr::select(-survived), type = "class")
log_tibble <- bind_cols(log_tibble, titanic_test %>% 
                              dplyr::select(survived))

# Getting the accuracy
log_acc <- augment(log_training_fit, new_data = titanic_test) %>% 
  accuracy(truth = survived, estimate = .pred_class)

results <- tibble(model = "Logistic Regression", accuracy = log_acc$.estimate)
results
```

The logistic regression model's average accuracy across folds was 0.7915259	and the testing accuracy is 0.7985075. These two numbers are very close because cross validation provides a better estimate of the testing accuracy.  

We usually observe that the testing accuracy is lower than the training accuracy because the model tends to do better on data it has been trained on (i.e. the cross validation or training data). However, in this case, the testing accuracy is higher than the average accuracy on the testing set of each of the folds. According to Professor Coburn's office hours, this may occur because some models are more suited to do better in general on new data. Also, it may be due to the titanic data being an easier data set to model (from lecture).   

